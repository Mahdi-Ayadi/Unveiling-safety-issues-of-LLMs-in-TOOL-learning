# Unveiling-safety-issues-of-LLMs-in-TOOL-learning
Research project reproducing and extending ToolSword attacks to evaluate LLM safety in tool learning. Includes parsing, prompting, and model evaluation pipelines for analyzing vulnerabilities and robustness.
